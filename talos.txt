Building scalabale applications with Erlang

(case of pubsub systems)

Why Erlang?
1) ease of prototyping
2) soft real-time
3) => low latency
4) reliable
5) fast enough

Commentator for onet.pl (heavy loading done by Talos)
Oortle (big cluster of erlang nodes)
WhatsApp.com (tuned ejabberd - 2.8mln of users per machine)

Commentator (Talos) case.
* few authors - many subscribers
* up to 1.5 mil of users
* cluster consisting of 3 machines (flat)
* load balancing is done via client
* state is synchronized with Mnesia (2 stage commit)
** scales up to 10 machines for this type of load

Oortle
* more authors - many subscribers
* up to 200 machines
* up to 1 mln of users per machine
* advanced load balancing
* automatic cluster resizing
* state is synchronized via custom protocol
* there are 2 roles in cluster
** control node
** worker node

WhatsApp.com
* IM application
** (many authors - many subscribers)
* ejabberd
* tuned up to 2.8 mil of users per machine

Erlang part.

Erlang was not designed to
* process text efficiently
* crunch numbers at lightning speed
* run in browser
Erlang was designed to
* run concurrent tasks
* run programs that never stop
** => 99.999% uptime (4 minutes of downtime per year)

Methods of achieving such reliability
* think "protocols"
* unit of isolation is a process
* let it crash (OTP, supervision trees)
* COP - concurrency oriented programming
** it is easy to step outside of your single processor -
   the world is concurrent just as your single processor Erlang program

Socket accept rate
There is a lock on OS for single socket. And it is not possible
to use more then one core to accept connection on single socket.

To utilize more cores you need to listen on more then one socket.


Mnesia - quite good when you use it for intended purpose
Pros:
* zero configuration
* blazing fast reads using mnesia:fun/2
Unanswered questions:
* removing dead node from cluster
* network split (think different cities/countries/continents)
* limit of scaling for single writer scenario

Pubsub-with-history in SMP setting
1) single pubsub process, guarding ets
  -> subscribes are too slow
2) direct access to ets (e.g. gproc)
  -> fast enough
  -> watch out for duplicates!
3) next step
  -> named public ets per scheduler
  -> phash2(pid(), n)
  -> up to 300k subscribes per second!

SockJS
* latency long tail (>60s)
* up to 100k per node
* lock congestion

Our rewrite:
* 1 active process per client (every protocol)
** no lock congestion on timers
* avoid multiple encoding of same data
* up to 700k per node
* stable, low latency
